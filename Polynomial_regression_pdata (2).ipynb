{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7fCTqNGFEmWx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d83il-E-E9P2"
   },
   "outputs": [],
   "source": [
    "file_path = 'Inputs_modified_a.xlsx'\n",
    "dataset=pd.read_excel(file_path)\n",
    "x=dataset.iloc[2:,2:-1].values\n",
    "y=dataset.iloc[2:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9WwnyTqvG3we",
    "outputId": "8de19dee-be68-4c1a-cbf5-d936bbdc36de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52.6 76.6 19.2 ... 0 600 2]\n",
      " [52.6 76.6 19.2 ... 0 600 2.3]\n",
      " [52.6 76.6 19.2 ... 0 900 1.8]\n",
      " ...\n",
      " [38.1 66 13.6 ... 0.2388 1500 1.8]\n",
      " [38.1 66 13.6 ... 0.2388 1500 2]\n",
      " [38.1 66 13.6 ... 0.2388 1500 2.3]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FD0lY7EMHx18",
    "outputId": "8a312085-2dc2-4fd8-df89-65e80e824bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.4900692 44.0212568 103.090293 98.0306171 92.0327738 120.695862\n",
      " 117.973042 114.246795 137.594684 137.103579 135.643063 119.083949\n",
      " 113.581613 107.325081 223.451568 215.423479 205.202528 256.163744\n",
      " 252.699618 247.067974 287.612614 288.357209 287.151583 54.5844441\n",
      " 47.6059779 40.1867174 146.405566 133.971542 120.409132 182.754255\n",
      " 174.44631 164.795386 217.872122 213.737578 208.129133 171.557512\n",
      " 168.832244 164.470379 280.128094 277.743054 272.585691 309.384675\n",
      " 310.907001 310.20901 338.032976 342.998919 346.221891 69.4333031\n",
      " 65.407838 60.9394313 140.059377 133.864298 126.318114 162.394769\n",
      " 159.285135 154.779523 183.880081 183.676551 182.167157 40.0930395\n",
      " 32.8929284 25.2785053 129.029184 115.98735 101.964396 165.852115\n",
      " 156.800686 146.568687 201.55345 196.573817 190.265646 25.7707222\n",
      " 18.6693525 11.1905385 105.637114 92.5380291 78.644788 142.324015\n",
      " 132.897781 122.514736 177.990661 172.341626 165.588549 37.5444388\n",
      " 35.0107133 32.2489806 77.5150042 73.2956993 68.4072638 91.7893871\n",
      " 89.3855542 86.2410137 105.464679 104.822267 103.441784 34.660393\n",
      " 31.5628256 28.2270584 81.745434 76.4721642 70.4958423 98.1588502\n",
      " 94.9503507 90.9434057 113.949136 112.773032 110.775566 120.131887\n",
      " 114.750256 108.575248 226.78835 219.22244 209.357918 258.71213 255.691912\n",
      " 250.451343 289.471344 290.60976 289.783426 41.06432 33.0562122 24.6620062\n",
      " 61.7411681 55.75979 49.3295577 82.0167363 78.1044857 73.6832886 89.105613\n",
      " 82.7712139 75.8897166 187.159551 176.569331 164.382343 222.258479\n",
      " 216.107539 208.178475 255.935806 254.109736 250.498069 71.8798731\n",
      " 65.48975 58.6333301 162.874704 151.668 139.188629 198.273513 191.207332\n",
      " 182.65337 232.259248 229.330898 224.823173 89.9585487 83.4646803\n",
      " 76.4292284 188.443435 177.397219 164.802728 225.107137 218.561667\n",
      " 210.264511 260.240894 258.114711 254.204202 29.4946876 22.0061827\n",
      " 14.1132885 116.725079 102.932516 88.2598906 155.363345 145.513322\n",
      " 134.599385 192.89526 187.095923 180.072041 106.856073 100.674633\n",
      " 93.8223432 214.504981 204.895265 193.248016 249.189531 244.316575\n",
      " 237.322116 282.505298 282.102055 279.700374 84.3057514 77.8213933\n",
      " 70.7989898 183.731155 172.858168 160.40702 218.824055 212.404501\n",
      " 204.219693 252.546856 250.477629 246.622992 56.7216437 53.2014247\n",
      " 49.343001 113.653219 107.907488 101.165725 133.643702 130.482295\n",
      " 126.239167 152.802692 152.125962 150.390953 83.3017218 77.4639798\n",
      " 71.1217494 173.676262 163.839586 152.539317 206.600921 200.878017\n",
      " 193.515704 238.141569 236.434472 233.073567 93.2355876 86.8453028\n",
      " 79.8900198 193.475026 182.832284 170.533621 229.259754 223.154376\n",
      " 215.213133 263.567588 261.869503 258.336833 134.987545 129.930059\n",
      " 123.996174 238.651989 231.668368 222.327352 271.139214 268.688257\n",
      " 264.002838 302.408825 304.065306 303.780146 50.9398213 46.1908476\n",
      " 41.1067295 117.017904 108.638951 99.3641941 143.226375 137.863419\n",
      " 131.445632 168.410629 166.074797 162.607543 110.846549 105.088383\n",
      " 98.6450947 213.797349 205.014769 194.196236 247.122796 242.902041\n",
      " 236.598869 279.118963 279.166532 277.285736 110.185037 104.206824\n",
      " 97.5645064 213.714301 204.33623 192.984384 248.44263 243.714498\n",
      " 236.921995 281.726104 281.388851 279.105685 242.951884 254.598005\n",
      " 261.768473 357.402978 362.749149 366.10086 388.376184 395.756944\n",
      " 401.775398 418.753733 428.298665 436.789023 72.2838873 65.7482165\n",
      " 58.7446198 163.485678 151.989226 139.236725 199.608807 192.291779\n",
      " 183.500211 234.315926 231.189335 226.490066 77.5750059 70.9359285\n",
      " 63.807184 171.265195 159.608876 146.629359 208.40248 201.063526\n",
      " 192.176408 244.041376 241.022484 236.362441 57.1057396 51.7292897\n",
      " 45.9663442 133.102516 123.733215 113.302594 161.997986 156.065675\n",
      " 148.898079 189.801348 187.29848 183.487636 32.1597258 27.6869427\n",
      " 22.9464478 87.0976975 78.9678142 70.2102987 111.119563 105.528513\n",
      " 99.1790648 134.335424 131.344405 127.500152 61.5390932 55.1673831\n",
      " 48.3431843 155.243533 144.144154 131.751636 188.890709 181.87977\n",
      " 173.366842 221.278348 218.337749 213.808985 22.1861716 14.3703955\n",
      " 6.15035839 108.6543 94.3302634 79.1613466 147.674115 137.313404\n",
      " 125.942326 185.709263 179.411665 171.957654 35.1425672 27.8965765\n",
      " 20.2649023 112.460638 99.1274495 85.0097921 151.023604 141.423265\n",
      " 130.879773 188.516799 182.781336 175.969614 44.2105264 36.972806\n",
      " 29.3207295 130.462033 117.235871 103.073153 168.96158 159.715678\n",
      " 149.336001 206.243295 201.085456 194.63993 53.2333091 46.4826342\n",
      " 39.3130266 139.02462 126.826388 113.607059 175.339491 167.079003\n",
      " 157.589337 210.392645 206.151997 200.546428 39.3421254 32.0202017\n",
      " 24.2898121 124.837838 111.514921 97.2811761 162.747851 153.36751\n",
      " 142.88996 199.541603 194.200449 187.61312 58.6840748 51.9867039\n",
      " 44.8597129 144.779988 132.751982 119.660305 181.066859 173.02169\n",
      " 163.701809 216.047131 212.047979 206.638749 55.7061578 48.6117633\n",
      " 41.0767343 146.062324 133.279007 119.416551 183.966135 175.320837\n",
      " 165.37918 220.561224 216.13012 210.258835 51.6659738 44.914809 37.7475816\n",
      " 137.164486 124.998118 111.816446 173.037546 164.7869 155.316244\n",
      " 207.687767 203.425819 197.806561 247.357653 240.292179 232.81267\n",
      " 340.204298 327.656277 314.065883 377.255276 368.771134 359.029788\n",
      " 413.083979 408.736237 402.984781 50.9724933 44.1807307 36.9762405\n",
      " 135.834233 123.562772 110.301487 172.086329 163.715925 154.153887\n",
      " 207.115876 202.726578 197.005852 39.1177908 31.7477501 23.9673878\n",
      " 123.052656 109.577352 95.2111738 161.580875 152.042072 141.428886\n",
      " 198.968286 193.474209 186.754611 39.4179034 32.860946 25.9268966\n",
      " 119.61819 107.823637 95.1329816 152.955471 144.793288 135.556533\n",
      " 185.30337 180.844238 175.185286 34.5454599 27.2251284 19.5030697\n",
      " 118.510672 105.078528 90.7763227 156.658261 147.108324 136.508077\n",
      " 193.693373 188.135735 181.377187 52.0187266 44.9859809 37.5323826\n",
      " 137.234778 124.455059 110.701106 175.207607 166.408317 156.432037\n",
      " 211.909257 207.188186 201.141811 32.0520992 16.6256354 69.6214875\n",
      " 57.740264 45.1868228 106.689837 98.4097233 89.411393 65.1693977\n",
      " 51.2052031 36.6021016 101.618327 91.1069012 79.8582708 137.401175\n",
      " 130.460405 122.714368 39.0113152 31.9194118 24.4275461 122.87272\n",
      " 109.940767 96.108011 159.831589 150.75295 140.587303 195.676965\n",
      " 190.553026 184.198794 72.556784 65.6829373 58.3276085 167.007606\n",
      " 154.956151 141.619395 204.205633 196.476519 187.240228 240.011371\n",
      " 236.614484 231.60607 75.8015429 69.0586153 61.8265134 170.652282\n",
      " 158.872419 145.763166 207.613087 200.170248 191.17593 243.133833\n",
      " 240.021994 235.263393 53.0790868 45.7624249 37.9892601 149.464599\n",
      " 136.342397 122.057952 187.544256 178.720905 168.507958 224.329225\n",
      " 219.86231 213.858588 37.6504238 22.8365023 7.43972207 76.0949717\n",
      " 64.6711163 52.603264 113.964735 106.055468 97.4667352 158.132176\n",
      " 153.294776 147.489838 256.505233 249.649814 240.422773 291.172186\n",
      " 288.929171 284.44228 324.456806 326.379167 326.36393 91.0131103\n",
      " 84.8787401 78.1703573 191.280214 181.457127 169.884159 224.24754\n",
      " 218.837245 211.549304 255.977371 254.794437 251.785857 129.62932\n",
      " 124.070447 117.770457 227.809729 219.292928 208.72683 262.615072\n",
      " 258.698729 252.696691 295.946534 296.316569 294.771835 75.1447873\n",
      " 68.4619632 61.2762741 174.744232 163.259057 150.322571 210.666245\n",
      " 203.593079 194.854726 245.198155 242.487828 238.044804 130.407381\n",
      " 124.285016 117.539583 217.678058 207.216312 195.174678 257.279917\n",
      " 251.388802 243.725461 295.081722 293.665442 290.476047 80.7848542\n",
      " 74.0918387 66.8969812 176.672717 165.062306 152.067975 213.726307\n",
      " 206.505098 197.680386 249.310148 246.455816 241.919081 123.052314\n",
      " 117.328395 110.860175 227.581216 219.004416 208.279974 261.880061\n",
      " 257.979451 251.917184 294.806567 295.248228 293.718174 127.103243\n",
      " 122.396789 116.725779 235.467027 229.851608 221.679233 264.317944\n",
      " 262.94273 259.305646 292.366495 294.815136 295.364374 53.9046554\n",
      " 49.3127166 44.3779809 119.270425 111.365203 102.510964 144.088565\n",
      " 139.167305 133.153438 167.971585 166.013301 162.914733 87.8358222\n",
      " 81.6187988 74.8783573 181.654324 171.22808 159.296031 216.041711\n",
      " 209.90824 202.087763 249.065307 247.130103 243.491943 12.4643444\n",
      " 2.30375506 26.3078738 16.5691551 6.2998618 100.808168 94.6007169\n",
      " 87.8066802 199.389087 189.14443 177.196651 235.089215 229.379152\n",
      " 221.80155 269.278667 267.950784 264.784502 72.0465307 67.5384145\n",
      " 62.6221696 141.045447 133.476645 124.753089 167.210396 162.872211\n",
      " 157.246183 192.253219 191.062886 188.582838 47.3024123 32.7054396\n",
      " 17.4992239 84.1791317 73.044474 61.2261457 120.465554 112.852891\n",
      " 104.485971 24.7942722 17.0895893 8.99350422 100.073694 85.8013208\n",
      " 70.7950056 140.921329 130.421218 119.052645 180.736875 174.138724\n",
      " 166.541559 64.373427 57.720862 50.6180612 154.064157 142.223101\n",
      " 129.210837 190.279344 182.545969 173.421513 225.130643 221.545913\n",
      " 216.451068 51.0129702 44.0593348 36.675364 142.524255 130.032353\n",
      " 116.449583 178.996084 170.581118 160.858808 214.222942 209.944732\n",
      " 204.217103 126.428908 122.860055 118.34801 215.264984 211.02923\n",
      " 204.604744 241.093112 240.48801 237.963632 266.138887 268.751219\n",
      " 269.781467 86.0295135 83.6043688 80.5348374 146.65583 143.79629\n",
      " 139.444484 164.182395 163.788049 162.088176 181.186567 182.9772\n",
      " 183.692632 76.0980658 69.3035185 62.0191166 170.912941 159.01806\n",
      " 145.800245 208.18156 200.639495 191.549647 244.002816 240.816006\n",
      " 235.984018 72.4680613 58.1368073 43.0574047 105.490023 94.881358\n",
      " 83.4133284 138.159622 131.289235 123.484764 99.3142896 92.4165408\n",
      " 84.8935651 206.510866 195.911258 183.357695 240.321946 234.372521\n",
      " 226.41609 273.189896 271.724079 268.326925 113.396283 107.766941\n",
      " 101.385297 218.167537 210.148506 199.903596 249.387055 245.859516\n",
      " 240.160908 279.582594 280.209767 278.890532 57.1980608 49.783011\n",
      " 41.9021354 124.887028 111.300468 96.7015223 158.689693 149.05531\n",
      " 138.150498 191.369237 185.863312 178.843946 42.2305619 36.2783329\n",
      " 29.9618589 118.693259 108.002589 96.4068984 149.644959 142.398004\n",
      " 134.067057 179.573213 175.824923 170.874127 73.5684156 68.4730961\n",
      " 62.9240493 172.40264 164.211847 154.688748 203.627728 199.123894\n",
      " 193.228421 233.730075 232.752042 230.439229 17.0370557 10.9982329\n",
      " 4.65812162 100.127855 89.0536209 77.3269801 136.024658 128.184882\n",
      " 119.578539 170.93279 166.390958 160.99919 71.8753593 65.8514198\n",
      " 59.3669308 158.987029 148.655231 137.030401 191.527882 185.16215\n",
      " 177.314467 222.818112 220.376505 216.394528 81.1466559 74.9821523\n",
      " 68.3101602 174.687108 164.253932 152.354279 208.622771 202.419404\n",
      " 194.562251 241.191025 239.133527 235.390648 101.62302 95.5107715\n",
      " 88.805024 197.666734 187.678588 175.988309 232.423647 226.89647\n",
      " 219.518801 265.7585 264.527417 261.483081 195.483167 194.333362\n",
      " 190.766833 295.672258 306.853934 304.284946 327.522785 339.726637\n",
      " 340.545843 356.966863 371.236606 375.793338 17.8865325 9.85391218\n",
      " 1.41891048 101.562476 87.0936346 71.8010685 139.34823 128.826424\n",
      " 117.338623 176.353693 169.850334 162.265296 47.8860983 38.0714492\n",
      " 27.784617 72.5724385 65.2341004 57.3485701 96.825749 92.0124502 86.588055]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UZGUXaDkpOYV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.85,test_size=0.15,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62zpq1hGOt9R",
    "outputId": "3918b60c-e1a7-4205-ef41-f866c50332c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x_train is: (831, 11)\n",
      "The shape of y_train is:  (831,)\n",
      "Number of training examples (m): 831\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of x_train is:', x_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-nyq2ggUWlT",
    "outputId": "11b6ccf2-64d6-4050-c6c9-4c820c1b0525"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train is numpy array: True\n",
      "y_train is numpy array: True\n"
     ]
    }
   ],
   "source": [
    "print(f'x_train is numpy array: {isinstance(x_train, np.ndarray)}')\n",
    "print(f'y_train is numpy array: {isinstance(y_train, np.ndarray)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "awOY78z5JVLG"
   },
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b,theta):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression with eleven variables using a for loop.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Shape (m, 11) Input to the model (four features of the training examples)\n",
    "        y (ndarray): Shape (m,) Label (Actual output values for the training examples)\n",
    "        w (ndarray): Shape (11,) Parameters of the model (weights for the four features)\n",
    "        b (scalar): Parameter of the model (bias)\n",
    "\n",
    "    Returns:\n",
    "        total_cost (float): The cost of using w, b as the parameters for linear regression\n",
    "                            to fit the data points in x and y.\n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    m,n = x.shape\n",
    "\n",
    "    # Initialize total cost\n",
    "    total_cost = 0\n",
    "    f_wb=np.zeros(m)\n",
    "\n",
    "    # Compute the total cost using a for loop\n",
    "    for i in range(m):\n",
    "        f_wb[i] = b  # Start with the bias term\n",
    "        for j in range(n):  # Sum over all features\n",
    "            f_wb[i] += w[j] * x[i, j] + theta[j] * x[i, j]**2\n",
    "        total_cost += (f_wb[i] - y[i]) ** 2  # Sum of squared errors\n",
    "\n",
    "    # Compute the average cost\n",
    "    total_cost = total_cost / (2 * m)\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZN0kJQKcKDo6",
    "outputId": "b46bb842-d8dd-418a-c344-3ba00501684a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "Cost at initial w: 15314.256\n"
     ]
    }
   ],
   "source": [
    "m,n=x_train.shape\n",
    "initial_w = np.zeros(n)\n",
    "initial_theta=np.zeros(n)\n",
    "initial_b =0.5\n",
    "\n",
    "cost = compute_cost(x_train, y_train, initial_w, initial_b,initial_theta)\n",
    "print(type(cost))\n",
    "print(f'Cost at initial w: {cost:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "S0CitG7CcG5a"
   },
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b,theta):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression with multiple variables.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Shape (m, n) Input to the model (features of the training examples)\n",
    "        y (ndarray): Shape (m,) Label (actual output values for the training examples)\n",
    "        w (ndarray): Shape (n,) Parameters of the model (weights for the features)\n",
    "        b (scalar): Parameter of the model (bias)\n",
    "\n",
    "    Returns:\n",
    "        dj_dw (ndarray): Shape (n,) The gradient of the cost w.r.t. the parameters w\n",
    "        dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    m, n = x.shape\n",
    "    total_cost = np.zeros(m)\n",
    "    # Initialize gradients\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_dtheta = np.zeros(n)\n",
    "    dj_db = 0.0\n",
    "    f_wb=np.zeros(m)\n",
    "    error=np.zeros(m)\n",
    "    for i in range(m):\n",
    "        f_wb[i] = np.dot(w.T,x[i,:]) + np.dot(theta.T,x[i,:]**2)+ b  # Predicted value\n",
    "        error[i] = f_wb[i] - y[i]  # Error term\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += error[i] * x[i, j]  # Gradient for w[j]\n",
    "            dj_dtheta[j] += error[i] * x[i, j]**2\n",
    "        dj_db += error[i]  # Gradient for b\n",
    "\n",
    "    # Average the gradients\n",
    "    dj_dw = (dj_dw) / m\n",
    "    dj_dtheta = (dj_dtheta) / m\n",
    "    dj_db= (dj_db)/ m\n",
    "\n",
    "\n",
    "    return dj_dw, dj_db,dj_dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9uvMegwcsWN",
    "outputId": "6cfbb7b0-477a-456e-c796-1b2169cd8a3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_dw,dj_dtheta at initial w, b,theta (zeros): [-1.93397960e+03 -1.18554702e+04 -2.84273910e+03 -8.46926755e+02\n",
      " -7.61888120e+03 -5.95703070e+03 -9.55185591e+02 -1.45760182e+02\n",
      " -2.04012262e+01 -1.82022002e+05 -3.14060370e+02]\n",
      "Gradient at initial w, b (zeros): [-1.93397960e+03 -1.18554702e+04 -2.84273910e+03 -8.46926755e+02\n",
      " -7.61888120e+03 -5.95703070e+03 -9.55185591e+02 -1.45760182e+02\n",
      " -2.04012262e+01 -1.82022002e+05 -3.14060370e+02] -155.44185656249084 [-4.38355644e+04 -9.09893076e+05 -5.56441850e+04 -1.02422923e+04\n",
      " -3.75076492e+05 -2.33642368e+05 -6.00612575e+03 -3.52355202e+02\n",
      " -8.18282434e+00 -2.27602571e+08 -6.41010305e+02]\n"
     ]
    }
   ],
   "source": [
    "m,n=x_train.shape\n",
    "initial_w = np.zeros(n)\n",
    "initial_theta=np.zeros(n)\n",
    "initial_b = 0\n",
    "\n",
    "tmp_dj_dw, tmp_dj_db,tmp_dj_dtheta = compute_gradient(x_train, y_train, initial_w, initial_b,initial_theta)\n",
    "print(f'dj_dw,dj_dtheta at initial w, b,theta (zeros): {tmp_dj_dw}')\n",
    "print('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db,tmp_dj_dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "mLUdICAjA2to"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in,theta_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
    "    num_iters gradient steps with learning rate alpha.\n",
    "\n",
    "    Args:\n",
    "        x : (ndarray): Shape (m, n) Input data (features of the training examples).\n",
    "        y : (ndarray): Shape (m,) Output data (actual output values for the training examples).\n",
    "        w_in : (ndarray): Shape (n,) Initial values of parameters (weights) of the linear term.\n",
    "        theta_in : ndarray): Shape (n,) Initial values of parameters (weights) of the quadratic term.\n",
    "         \n",
    "        b_in : (scalar) Initial value of parameter (bias) of the model.\n",
    "        cost_function : function to compute cost.\n",
    "        gradient_function : function to compute the gradient.\n",
    "        alpha : (float) Learning rate.\n",
    "        num_iters : (int) Number of iterations to run gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        w : (ndarray): Shape (n,) Updated values of parameters (weights) of the model after running gradient descent.\n",
    "        b : (scalar) Updated value of parameter (bias) of the model after running gradient descent.\n",
    "        J_history : List of costs at each iteration.\n",
    "        w_history : List of weights at each iteration.\n",
    "    \"\"\"\n",
    "    m, n = x.shape\n",
    "    # Initialize parameters\n",
    "    w = copy.deepcopy(w_in)  # Avoid modifying global w within function\n",
    "    theta=copy.deepcopy(theta_in)\n",
    "    b = b_in\n",
    "\n",
    "    # History of cost and weights\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_dw, dj_db,dj_dtheta = gradient_function(x, y, w, b,theta)\n",
    "\n",
    "        # Update Parameters using w, theta, b, alpha, and gradient\n",
    "        for j in range(n):\n",
    "            w[j] = w[j] - alpha * dj_dw[j]\n",
    "            theta[j] = theta[j] - alpha * dj_dtheta[j]\n",
    "            b = b - alpha * dj_db\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 10000000:  # Prevent resource exhaustion\n",
    "            cost = cost_function(x, y, w, b,theta)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 100) == 0:\n",
    "            w_history.append(w.copy())\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):15.2f}\")\n",
    "\n",
    "    return w, b,theta, J_history, w_history  # Return w and J, w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ns8IIXupA8XK",
    "outputId": "c9c74352-2bc4-48d5-f601-1b4188d2c411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost         5601.34\n",
      "Iteration   50: Cost         2750.22\n",
      "Iteration  100: Cost         2749.67\n",
      "Iteration  150: Cost         2749.13\n",
      "Iteration  200: Cost         2748.58\n",
      "Iteration  250: Cost         2748.03\n",
      "Iteration  300: Cost         2747.49\n",
      "Iteration  350: Cost         2746.94\n",
      "Iteration  400: Cost         2746.40\n",
      "Iteration  450: Cost         2745.85\n",
      "Iteration  500: Cost         2745.31\n",
      "Iteration  550: Cost         2744.76\n",
      "Iteration  600: Cost         2744.22\n",
      "Iteration  650: Cost         2743.68\n",
      "Iteration  700: Cost         2743.13\n",
      "Iteration  750: Cost         2742.59\n",
      "Iteration  800: Cost         2742.05\n",
      "Iteration  850: Cost         2741.51\n",
      "Iteration  900: Cost         2740.97\n",
      "Iteration  950: Cost         2740.43\n",
      "Iteration 1000: Cost         2739.89\n",
      "Iteration 1050: Cost         2739.35\n",
      "Iteration 1100: Cost         2738.81\n",
      "Iteration 1150: Cost         2738.27\n",
      "Iteration 1200: Cost         2737.73\n",
      "Iteration 1250: Cost         2737.19\n",
      "Iteration 1300: Cost         2736.65\n",
      "Iteration 1350: Cost         2736.12\n",
      "Iteration 1400: Cost         2735.58\n",
      "Iteration 1450: Cost         2735.04\n",
      "Iteration 1500: Cost         2734.51\n",
      "Iteration 1550: Cost         2733.97\n",
      "Iteration 1600: Cost         2733.44\n",
      "Iteration 1650: Cost         2732.90\n",
      "Iteration 1700: Cost         2732.37\n",
      "Iteration 1750: Cost         2731.84\n",
      "Iteration 1800: Cost         2731.30\n",
      "Iteration 1850: Cost         2730.77\n",
      "Iteration 1900: Cost         2730.24\n",
      "Iteration 1950: Cost         2729.71\n",
      "Iteration 2000: Cost         2729.18\n",
      "Iteration 2050: Cost         2728.64\n",
      "Iteration 2100: Cost         2728.11\n",
      "Iteration 2150: Cost         2727.58\n",
      "Iteration 2200: Cost         2727.05\n",
      "Iteration 2250: Cost         2726.52\n",
      "Iteration 2300: Cost         2726.00\n",
      "Iteration 2350: Cost         2725.47\n",
      "Iteration 2400: Cost         2724.94\n",
      "Iteration 2450: Cost         2724.41\n",
      "Iteration 2500: Cost         2723.89\n",
      "Iteration 2550: Cost         2723.36\n",
      "Iteration 2600: Cost         2722.83\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Perform gradient descent\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m w, b, theta,_, _ \u001b[38;5;241m=\u001b[39m gradient_descent(x_train, y_train, initial_w, initial_b,initial_theta, compute_cost, compute_gradient, alpha, iterations)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw, b found by gradient descent:\u001b[39m\u001b[38;5;124m\"\u001b[39m, w, b,theta)\n",
      "Cell \u001b[1;32mIn[28], line 36\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(x, y, w_in, b_in, theta_in, cost_function, gradient_function, alpha, num_iters)\u001b[0m\n\u001b[0;32m     32\u001b[0m w_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Calculate the gradient and update the parameters\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     dj_dw, dj_db,dj_dtheta \u001b[38;5;241m=\u001b[39m gradient_function(x, y, w, b,theta)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Update Parameters using w, theta, b, alpha, and gradient\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n",
      "Cell \u001b[1;32mIn[26], line 25\u001b[0m, in \u001b[0;36mcompute_gradient\u001b[1;34m(x, y, w, b, theta)\u001b[0m\n\u001b[0;32m     23\u001b[0m error\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m):\n\u001b[1;32m---> 25\u001b[0m     f_wb[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(w\u001b[38;5;241m.\u001b[39mT,x[i,:]) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(theta\u001b[38;5;241m.\u001b[39mT,x[i,:]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m+\u001b[39m b  \u001b[38;5;66;03m# Predicted value\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     error[i] \u001b[38;5;241m=\u001b[39m f_wb[i] \u001b[38;5;241m-\u001b[39m y[i]  \u001b[38;5;66;03m# Error term\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "m,n=x_train.shape\n",
    "initial_w = np.zeros(n)\n",
    "initial_theta = np.zeros(n)\n",
    "initial_b=0.5\n",
    "# Define the learning rate and number of iterations\n",
    "alpha = 0.00000000000072\n",
    "iterations = 5000\n",
    "# Perform gradient descent\n",
    "w, b, theta,_, _ = gradient_descent(x_train, y_train, initial_w, initial_b,initial_theta, compute_cost, compute_gradient, alpha, iterations)\n",
    "\n",
    "print(\"w, b found by gradient descent:\", w, b,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "wt_tfNUsWfvA",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x_test is: (147, 11)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of x_test is:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "9MSyCWFmVCil"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-27048.968615780097 248.44263\n",
      "-92889.2820218933 68.3101602\n",
      "-14845.509205177448 377.255276\n",
      "-22114.06729110934 167.079003\n",
      "50855.69966660773 240.816006\n",
      "-85896.91918983901 17.0370557\n",
      "-53770.03258716129 205.014769\n",
      "-64450.91283288742 107.907488\n",
      "-6112.389880521395 262.94273\n",
      "-57215.223101681404 117.235871\n",
      "-80366.15412823806 74.8783573\n",
      "-86558.91997120366 61.2762741\n",
      "-59695.549120355565 223.451568\n",
      "9648.469600202881 128.826424\n",
      "-37999.057966542445 199.903596\n",
      "39057.51190470565 216.394528\n",
      "-12528.301921533433 163.715925\n",
      "-73143.5634655371 127.103243\n",
      "-22919.07631707779 162.872211\n",
      "-46916.35338715643 133.971542\n",
      "-18121.237094099786 168.96158\n",
      "35591.13956392869 279.166532\n",
      "-13648.703838376037 154.779523\n",
      "28337.46689275947 240.321946\n",
      "-92375.86915659346 49.343001\n",
      "34563.23481702278 209.944732\n",
      "20348.16601128028 236.362441\n",
      "-96943.6330349321 63.807184\n",
      "30406.789371334307 180.072041\n",
      "-7813.4164670026075 164.795386\n",
      "-11206.393897646878 177.314467\n",
      "-84811.92230550319 34.5454599\n",
      "41397.91112802738 436.789023\n",
      "-57209.552530265035 103.073153\n",
      "-14262.90278721701 166.408317\n",
      "-22399.212425726582 90.9434057\n",
      "-47827.94309706542 109.940767\n",
      "-24978.199045271707 241.093112\n",
      "35421.676938330915 408.736237\n",
      "-61703.46329629803 197.666734\n",
      "-14836.558292605245 359.029788\n",
      "47827.07838958155 137.401175\n",
      "-81694.98891086734 98.6450947\n",
      "-92806.73020689504 58.7446198\n",
      "25618.224677210575 342.998919\n",
      "20627.052915410175 327.522785\n",
      "-15739.480294736983 142.324015\n",
      "-100794.97272602342 65.48975\n",
      "-86761.25818898756 32.1597258\n",
      "-29650.53793419612 137.863419\n",
      "-91333.10803756403 134.987545\n",
      "-82261.32260288512 114.750256\n",
      "-14672.766238423834 242.902041\n",
      "32476.297243460303 295.248228\n",
      "-80375.10504081027 87.8358222\n",
      "-4633.961498229951 137.313404\n",
      "-100789.30215460705 58.6333301\n",
      "37072.13457574292 170.874127\n",
      "-11621.921911409852 252.696691\n",
      "42750.274620491764 166.541559\n",
      "-72220.0007098996 49.3127166\n",
      "-38798.016043660566 169.884159\n",
      "-5416.510662949993 141.428886\n",
      "-61878.01973237657 146.65583\n",
      "30533.821198380672 131.344405\n",
      "27492.103331285616 183.692632\n",
      "-63399.20080128577 231.668368\n",
      "-97211.874834133 57.720862\n",
      "-2436.827412770982 101.618327\n",
      "59912.37540255541 169.850334\n",
      "-53353.2659419455 22.8365023\n",
      "-14676.04657957967 247.122796\n",
      "-81700.6594822837 105.088383\n",
      "30870.48455301221 241.919081\n",
      "42741.32370791956 180.736875\n",
      "-71656.18417067346 6.15035839\n",
      "-81870.12210788149 240.292179\n",
      "30861.533640440008 249.310148\n",
      "-89427.10566958645 28.2270584\n",
      "-14259.279934363884 76.0949717\n",
      "-24969.248132699504 237.963632\n",
      "-19531.026727343793 194.854726\n",
      "-6568.930987433917 165.852115\n",
      "-29915.73979107225 192.176408\n",
      "-43520.67104959462 111.514921\n",
      "-72484.80201143617 117.539583\n",
      "6817.968683679856 61.7411681\n",
      "8880.457826090336 72.4680613\n",
      "-5422.18123436636 152.042072\n",
      "-60579.54756861024 132.751982\n",
      "35596.81013534506 277.285736\n",
      "-50724.85883156367 219.292928\n",
      "-4637.241839385789 147.674115\n",
      "-27626.872874702807 173.366842\n",
      "-46610.89753059822 70.7950056\n",
      "-14257.232215800643 156.432037\n",
      "-73593.54389013794 32.8929284\n",
      "50852.4193254519 244.002816\n",
      "41417.69505158745 231.60607\n",
      "-17790.888900048063 261.880061\n",
      "-68747.80428293358 108.638951\n",
      "-59180.64468003198 96.7015223\n",
      "34095.67046018399 183.487636\n",
      "-6115.670221677233 264.317944\n",
      "-66728.64293878298 78.1703573\n",
      "-24974.91870411587 240.48801\n",
      "-31259.442759552076 229.259754\n",
      "-8846.21075076508 187.240228\n",
      "-68751.08462408942 117.017904\n",
      "-76989.34127375227 62.9240493\n",
      "-15703.951326485585 178.996084\n",
      "-58821.68038129294 70.2102987\n",
      "-59189.595592604186 124.887028\n",
      "-96678.43117805598 46.1908476\n",
      "-36731.11318201494 119.61819\n",
      "33421.14363480399 137.594684\n",
      "20630.333256566013 339.726637\n",
      "-89625.13927884825 88.805024\n",
      "-22408.163338298786 98.1588502\n",
      "17050.987983687715 69.6214875\n",
      "36007.90620914448 106.055468\n",
      "-4628.2909268135845 125.942326\n",
      "-46391.889415877486 190.766833\n",
      "-56343.69597566969 99.1274495\n",
      "-7513.631181860765 119.052645\n",
      "25967.64192122058 303.780146\n",
      "-63402.481142441604 238.651989\n",
      "-5465.859680148519 257.279917\n",
      "78610.3236076842 268.326925\n",
      "-13654.374409792405 159.285135\n",
      "42450.48933534992 208.129133\n",
      "25294.657669653025 269.781467\n",
      "-7819.087038418974 174.44631\n",
      "30727.20850359237 242.487828\n",
      "-62019.62300697108 141.045447\n",
      "-80638.50281515432 61.8265134\n",
      "-91997.14137655936 118.34801\n",
      "-24310.314098631625 38.0714492\n",
      "-10756.519114822147 195.911258\n",
      "31487.666082889627 197.806561\n",
      "20636.00382798238 340.545843\n",
      "-69018.67671122607 159.608876\n",
      "-84818.78214390794 123.052314\n",
      "-18466.93309217144 306.853934\n",
      "-79556.1951653933 44.1807307\n",
      "36925.644917974336 243.491943\n",
      "-89946.96956093765 67.5384145\n",
      "Mean Squared Error: 2689765358.591605\n",
      "Root Mean Squared Error: 51862.94783939306\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "k=x_test.shape[0]\n",
    "data=[]\n",
    "f_wb=np.zeros(k)\n",
    "y_pred=np.zeros(k)\n",
    "for i in range(k):\n",
    "        f_wb[i] = b  # Start with the bias term\n",
    "        \n",
    "        for j in range(11):  # Sum over all features\n",
    "            f_wb[i] += w[j] * x_test[i, j]+theta[j] * x_test[i, j]**2\n",
    "        print(f_wb[i], y_test[i])\n",
    "        row= [f_wb[i],y_test[i]]\n",
    "        data.append(row)\n",
    "        y_pred[i]=f_wb[i]\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data,columns=['Predicted','Actual'])\n",
    "file_name='Result_Polynomiar_Regression2.xlsx'\n",
    "df.to_excel(file_name, index=False)\n",
    "FileLink(file_name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
