{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7fCTqNGFEmWx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d83il-E-E9P2"
   },
   "outputs": [],
   "source": [
    "file_path = 'Inputs.xlsx'\n",
    "dataset=pd.read_excel(file_path)\n",
    "x=dataset.iloc[2:,2:-1].values\n",
    "y=dataset.iloc[2:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9WwnyTqvG3we",
    "outputId": "8de19dee-be68-4c1a-cbf5-d936bbdc36de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52.6 76.6 19.2 ... 0 600 2]\n",
      " [52.6 76.6 19.2 ... 0 600 2.3]\n",
      " [52.6 76.6 19.2 ... 0 900 1.8]\n",
      " ...\n",
      " [38.1 66 13.6 ... 0.2388 1500 1.8]\n",
      " [38.1 66 13.6 ... 0.2388 1500 2]\n",
      " [38.1 66 13.6 ... 0.2388 1500 2.3]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FD0lY7EMHx18",
    "outputId": "8a312085-2dc2-4fd8-df89-65e80e824bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.4900692 44.0212568 103.090293 98.0306171 92.0327738 120.695862\n",
      " 117.973042 114.246795 137.594684 137.103579 135.643063 119.083949\n",
      " 113.581613 107.325081 223.451568 215.423479 205.202528 256.163744\n",
      " 252.699618 247.067974 287.612614 288.357209 287.151583 54.5844441\n",
      " 47.6059779 40.1867174 146.405566 133.971542 120.409132 182.754255\n",
      " 174.44631 164.795386 217.872122 213.737578 208.129133 171.557512\n",
      " 168.832244 164.470379 280.128094 277.743054 272.585691 309.384675\n",
      " 310.907001 310.20901 338.032976 342.998919 346.221891 69.4333031\n",
      " 65.407838 60.9394313 140.059377 133.864298 126.318114 162.394769\n",
      " 159.285135 154.779523 183.880081 183.676551 182.167157 40.0930395\n",
      " 32.8929284 25.2785053 129.029184 115.98735 101.964396 165.852115\n",
      " 156.800686 146.568687 201.55345 196.573817 190.265646 25.7707222\n",
      " 18.6693525 11.1905385 105.637114 92.5380291 78.644788 142.324015\n",
      " 132.897781 122.514736 177.990661 172.341626 165.588549 37.5444388\n",
      " 35.0107133 32.2489806 77.5150042 73.2956993 68.4072638 91.7893871\n",
      " 89.3855542 86.2410137 105.464679 104.822267 103.441784 34.660393\n",
      " 31.5628256 28.2270584 81.745434 76.4721642 70.4958423 98.1588502\n",
      " 94.9503507 90.9434057 113.949136 112.773032 110.775566 120.131887\n",
      " 114.750256 108.575248 226.78835 219.22244 209.357918 258.71213 255.691912\n",
      " 250.451343 289.471344 290.60976 289.783426 41.06432 33.0562122 24.6620062\n",
      " 61.7411681 55.75979 49.3295577 82.0167363 78.1044857 73.6832886 89.105613\n",
      " 82.7712139 75.8897166 187.159551 176.569331 164.382343 222.258479\n",
      " 216.107539 208.178475 255.935806 254.109736 250.498069 71.8798731\n",
      " 65.48975 58.6333301 162.874704 151.668 139.188629 198.273513 191.207332\n",
      " 182.65337 232.259248 229.330898 224.823173 89.9585487 83.4646803\n",
      " 76.4292284 188.443435 177.397219 164.802728 225.107137 218.561667\n",
      " 210.264511 260.240894 258.114711 254.204202 29.4946876 22.0061827\n",
      " 14.1132885 116.725079 102.932516 88.2598906 155.363345 145.513322\n",
      " 134.599385 192.89526 187.095923 180.072041 106.856073 100.674633\n",
      " 93.8223432 214.504981 204.895265 193.248016 249.189531 244.316575\n",
      " 237.322116 282.505298 282.102055 279.700374 84.3057514 77.8213933\n",
      " 70.7989898 183.731155 172.858168 160.40702 218.824055 212.404501\n",
      " 204.219693 252.546856 250.477629 246.622992 56.7216437 53.2014247\n",
      " 49.343001 113.653219 107.907488 101.165725 133.643702 130.482295\n",
      " 126.239167 152.802692 152.125962 150.390953 83.3017218 77.4639798\n",
      " 71.1217494 173.676262 163.839586 152.539317 206.600921 200.878017\n",
      " 193.515704 238.141569 236.434472 233.073567 93.2355876 86.8453028\n",
      " 79.8900198 193.475026 182.832284 170.533621 229.259754 223.154376\n",
      " 215.213133 263.567588 261.869503 258.336833 134.987545 129.930059\n",
      " 123.996174 238.651989 231.668368 222.327352 271.139214 268.688257\n",
      " 264.002838 302.408825 304.065306 303.780146 50.9398213 46.1908476\n",
      " 41.1067295 117.017904 108.638951 99.3641941 143.226375 137.863419\n",
      " 131.445632 168.410629 166.074797 162.607543 110.846549 105.088383\n",
      " 98.6450947 213.797349 205.014769 194.196236 247.122796 242.902041\n",
      " 236.598869 279.118963 279.166532 277.285736 110.185037 104.206824\n",
      " 97.5645064 213.714301 204.33623 192.984384 248.44263 243.714498\n",
      " 236.921995 281.726104 281.388851 279.105685 242.951884 254.598005\n",
      " 261.768473 357.402978 362.749149 366.10086 388.376184 395.756944\n",
      " 401.775398 418.753733 428.298665 436.789023 72.2838873 65.7482165\n",
      " 58.7446198 163.485678 151.989226 139.236725 199.608807 192.291779\n",
      " 183.500211 234.315926 231.189335 226.490066 77.5750059 70.9359285\n",
      " 63.807184 171.265195 159.608876 146.629359 208.40248 201.063526\n",
      " 192.176408 244.041376 241.022484 236.362441 57.1057396 51.7292897\n",
      " 45.9663442 133.102516 123.733215 113.302594 161.997986 156.065675\n",
      " 148.898079 189.801348 187.29848 183.487636 32.1597258 27.6869427\n",
      " 22.9464478 87.0976975 78.9678142 70.2102987 111.119563 105.528513\n",
      " 99.1790648 134.335424 131.344405 127.500152 61.5390932 55.1673831\n",
      " 48.3431843 155.243533 144.144154 131.751636 188.890709 181.87977\n",
      " 173.366842 221.278348 218.337749 213.808985 22.1861716 14.3703955\n",
      " 6.15035839 108.6543 94.3302634 79.1613466 147.674115 137.313404\n",
      " 125.942326 185.709263 179.411665 171.957654 35.1425672 27.8965765\n",
      " 20.2649023 112.460638 99.1274495 85.0097921 151.023604 141.423265\n",
      " 130.879773 188.516799 182.781336 175.969614 44.2105264 36.972806\n",
      " 29.3207295 130.462033 117.235871 103.073153 168.96158 159.715678\n",
      " 149.336001 206.243295 201.085456 194.63993 53.2333091 46.4826342\n",
      " 39.3130266 139.02462 126.826388 113.607059 175.339491 167.079003\n",
      " 157.589337 210.392645 206.151997 200.546428 39.3421254 32.0202017\n",
      " 24.2898121 124.837838 111.514921 97.2811761 162.747851 153.36751\n",
      " 142.88996 199.541603 194.200449 187.61312 58.6840748 51.9867039\n",
      " 44.8597129 144.779988 132.751982 119.660305 181.066859 173.02169\n",
      " 163.701809 216.047131 212.047979 206.638749 55.7061578 48.6117633\n",
      " 41.0767343 146.062324 133.279007 119.416551 183.966135 175.320837\n",
      " 165.37918 220.561224 216.13012 210.258835 51.6659738 44.914809 37.7475816\n",
      " 137.164486 124.998118 111.816446 173.037546 164.7869 155.316244\n",
      " 207.687767 203.425819 197.806561 247.357653 240.292179 232.81267\n",
      " 340.204298 327.656277 314.065883 377.255276 368.771134 359.029788\n",
      " 413.083979 408.736237 402.984781 50.9724933 44.1807307 36.9762405\n",
      " 135.834233 123.562772 110.301487 172.086329 163.715925 154.153887\n",
      " 207.115876 202.726578 197.005852 39.1177908 31.7477501 23.9673878\n",
      " 123.052656 109.577352 95.2111738 161.580875 152.042072 141.428886\n",
      " 198.968286 193.474209 186.754611 39.4179034 32.860946 25.9268966\n",
      " 119.61819 107.823637 95.1329816 152.955471 144.793288 135.556533\n",
      " 185.30337 180.844238 175.185286 34.5454599 27.2251284 19.5030697\n",
      " 118.510672 105.078528 90.7763227 156.658261 147.108324 136.508077\n",
      " 193.693373 188.135735 181.377187 52.0187266 44.9859809 37.5323826\n",
      " 137.234778 124.455059 110.701106 175.207607 166.408317 156.432037\n",
      " 211.909257 207.188186 201.141811 0 0 0 32.0520992 16.6256354 0.589855236\n",
      " 69.6214875 57.740264 45.1868228 106.689837 98.4097233 89.411393\n",
      " 65.1693977 51.2052031 36.6021016 101.618327 91.1069012 79.8582708\n",
      " 137.401175 130.460405 122.714368 39.0113152 31.9194118 24.4275461\n",
      " 122.87272 109.940767 96.108011 159.831589 150.75295 140.587303 195.676965\n",
      " 190.553026 184.198794 72.556784 65.6829373 58.3276085 167.007606\n",
      " 154.956151 141.619395 204.205633 196.476519 187.240228 240.011371\n",
      " 236.614484 231.60607 75.8015429 69.0586153 61.8265134 170.652282\n",
      " 158.872419 145.763166 207.613087 200.170248 191.17593 243.133833\n",
      " 240.021994 235.263393 53.0790868 45.7624249 37.9892601 149.464599\n",
      " 136.342397 122.057952 187.544256 178.720905 168.507958 224.329225\n",
      " 219.86231 213.858588 37.6504238 22.8365023 7.43972207 76.0949717\n",
      " 64.6711163 52.603264 113.964735 106.055468 97.4667352 158.132176\n",
      " 153.294776 147.489838 256.505233 249.649814 240.422773 291.172186\n",
      " 288.929171 284.44228 324.456806 326.379167 326.36393 91.0131103\n",
      " 84.8787401 78.1703573 191.280214 181.457127 169.884159 224.24754\n",
      " 218.837245 211.549304 255.977371 254.794437 251.785857 129.62932\n",
      " 124.070447 117.770457 227.809729 219.292928 208.72683 262.615072\n",
      " 258.698729 252.696691 295.946534 296.316569 294.771835 75.1447873\n",
      " 68.4619632 61.2762741 174.744232 163.259057 150.322571 210.666245\n",
      " 203.593079 194.854726 245.198155 242.487828 238.044804 130.407381\n",
      " 124.285016 117.539583 217.678058 207.216312 195.174678 257.279917\n",
      " 251.388802 243.725461 295.081722 293.665442 290.476047 80.7848542\n",
      " 74.0918387 66.8969812 176.672717 165.062306 152.067975 213.726307\n",
      " 206.505098 197.680386 249.310148 246.455816 241.919081 123.052314\n",
      " 117.328395 110.860175 227.581216 219.004416 208.279974 261.880061\n",
      " 257.979451 251.917184 294.806567 295.248228 293.718174 127.103243\n",
      " 122.396789 116.725779 235.467027 229.851608 221.679233 264.317944\n",
      " 262.94273 259.305646 292.366495 294.815136 295.364374 53.9046554\n",
      " 49.3127166 44.3779809 119.270425 111.365203 102.510964 144.088565\n",
      " 139.167305 133.153438 167.971585 166.013301 162.914733 87.8358222\n",
      " 81.6187988 74.8783573 181.654324 171.22808 159.296031 216.041711\n",
      " 209.90824 202.087763 249.065307 247.130103 243.491943 12.4643444\n",
      " 2.30375506 0 26.3078738 16.5691551 6.2998618 100.808168 94.6007169\n",
      " 87.8066802 199.389087 189.14443 177.196651 235.089215 229.379152\n",
      " 221.80155 269.278667 267.950784 264.784502 72.0465307 67.5384145\n",
      " 62.6221696 141.045447 133.476645 124.753089 167.210396 162.872211\n",
      " 157.246183 192.253219 191.062886 188.582838 47.3024123 32.7054396\n",
      " 17.4992239 84.1791317 73.044474 61.2261457 120.465554 112.852891\n",
      " 104.485971 24.7942722 17.0895893 8.99350422 100.073694 85.8013208\n",
      " 70.7950056 140.921329 130.421218 119.052645 180.736875 174.138724\n",
      " 166.541559 64.373427 57.720862 50.6180612 154.064157 142.223101\n",
      " 129.210837 190.279344 182.545969 173.421513 225.130643 221.545913\n",
      " 216.451068 51.0129702 44.0593348 36.675364 142.524255 130.032353\n",
      " 116.449583 178.996084 170.581118 160.858808 214.222942 209.944732\n",
      " 204.217103 126.428908 122.860055 118.34801 215.264984 211.02923\n",
      " 204.604744 241.093112 240.48801 237.963632 266.138887 268.751219\n",
      " 269.781467 86.0295135 83.6043688 80.5348374 146.65583 143.79629\n",
      " 139.444484 164.182395 163.788049 162.088176 181.186567 182.9772\n",
      " 183.692632 76.0980658 69.3035185 62.0191166 170.912941 159.01806\n",
      " 145.800245 208.18156 200.639495 191.549647 244.002816 240.816006\n",
      " 235.984018 72.4680613 58.1368073 43.0574047 105.490023 94.881358\n",
      " 83.4133284 138.159622 131.289235 123.484764 99.3142896 92.4165408\n",
      " 84.8935651 206.510866 195.911258 183.357695 240.321946 234.372521\n",
      " 226.41609 273.189896 271.724079 268.326925 113.396283 107.766941\n",
      " 101.385297 218.167537 210.148506 199.903596 249.387055 245.859516\n",
      " 240.160908 279.582594 280.209767 278.890532 57.1980608 49.783011\n",
      " 41.9021354 124.887028 111.300468 96.7015223 158.689693 149.05531\n",
      " 138.150498 191.369237 185.863312 178.843946 42.2305619 36.2783329\n",
      " 29.9618589 118.693259 108.002589 96.4068984 149.644959 142.398004\n",
      " 134.067057 179.573213 175.824923 170.874127 73.5684156 68.4730961\n",
      " 62.9240493 172.40264 164.211847 154.688748 203.627728 199.123894\n",
      " 193.228421 233.730075 232.752042 230.439229 17.0370557 10.9982329\n",
      " 4.65812162 100.127855 89.0536209 77.3269801 136.024658 128.184882\n",
      " 119.578539 170.93279 166.390958 160.99919 71.8753593 65.8514198\n",
      " 59.3669308 158.987029 148.655231 137.030401 191.527882 185.16215\n",
      " 177.314467 222.818112 220.376505 216.394528 81.1466559 74.9821523\n",
      " 68.3101602 174.687108 164.253932 152.354279 208.622771 202.419404\n",
      " 194.562251 241.191025 239.133527 235.390648 101.62302 95.5107715\n",
      " 88.805024 197.666734 187.678588 175.988309 232.423647 226.89647\n",
      " 219.518801 265.7585 264.527417 261.483081 195.483167 194.333362\n",
      " 190.766833 295.672258 306.853934 304.284946 327.522785 339.726637\n",
      " 340.545843 356.966863 371.236606 375.793338 17.8865325 9.85391218\n",
      " 1.41891048 101.562476 87.0936346 71.8010685 139.34823 128.826424\n",
      " 117.338623 176.353693 169.850334 162.265296 47.8860983 38.0714492\n",
      " 27.784617 72.5724385 65.2341004 57.3485701 96.825749 92.0124502 86.588055]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UZGUXaDkpOYV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.8,test_size=0.2,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62zpq1hGOt9R",
    "outputId": "3918b60c-e1a7-4205-ef41-f866c50332c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x_train is: (786, 11)\n",
      "The shape of y_train is:  (786,)\n",
      "Number of training examples (m): 786\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of x_train is:', x_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-nyq2ggUWlT",
    "outputId": "11b6ccf2-64d6-4050-c6c9-4c820c1b0525"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train is numpy array: True\n",
      "y_train is numpy array: True\n"
     ]
    }
   ],
   "source": [
    "print(f'x_train is numpy array: {isinstance(x_train, np.ndarray)}')\n",
    "print(f'y_train is numpy array: {isinstance(y_train, np.ndarray)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "awOY78z5JVLG"
   },
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b,theta,beta):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression with eleven variables using a for loop.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Shape (m, 11) Input to the model (four features of the training examples)\n",
    "        y (ndarray): Shape (m,) Label (Actual output values for the training examples)\n",
    "        w (ndarray): Shape (11,) Parameters of the model (weights for the four features)\n",
    "        b (scalar): Parameter of the model (bias)\n",
    "\n",
    "    Returns:\n",
    "        total_cost (float): The cost of using w, b as the parameters for linear regression\n",
    "                            to fit the data points in x and y.\n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    m,n = x.shape\n",
    "\n",
    "    # Initialize total cost\n",
    "    total_cost = 0\n",
    "\n",
    "    # Compute the total cost using a for loop\n",
    "    for i in range(m):\n",
    "        f_wb_i = b  # Start with the bias term\n",
    "        for j in range(n):  # Sum over all features\n",
    "            f_wb_i += w[j] * x[i, j] + theta[j] * x[i, j]**2 + beta[j] * x[i, j]**3\n",
    "        total_cost += (f_wb_i - y[i]) ** 2  # Sum of squared errors\n",
    "\n",
    "    # Compute the average cost\n",
    "    total_cost = total_cost / (2 * m)\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZN0kJQKcKDo6",
    "outputId": "b46bb842-d8dd-418a-c344-3ba00501684a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "Cost at initial w: 15492.574\n"
     ]
    }
   ],
   "source": [
    "m,n=x_train.shape\n",
    "initial_w = np.zeros(n)\n",
    "initial_theta=np.zeros(n)\n",
    "initial_beta=np.zeros(n)\n",
    "initial_b =0.5\n",
    "\n",
    "cost = compute_cost(x_train, y_train, initial_w, initial_b,initial_theta,initial_beta)\n",
    "print(type(cost))\n",
    "print(f'Cost at initial w: {cost:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "S0CitG7CcG5a"
   },
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b,theta,beta):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression with multiple variables.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Shape (m, n) Input to the model (features of the training examples)\n",
    "        y (ndarray): Shape (m,) Label (actual output values for the training examples)\n",
    "        w (ndarray): Shape (n,) Parameters of the model (weights for the features)\n",
    "        b (scalar): Parameter of the model (bias)\n",
    "\n",
    "    Returns:\n",
    "        dj_dw (ndarray): Shape (n,) The gradient of the cost w.r.t. the parameters w\n",
    "        dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    m, n = x.shape\n",
    "  \n",
    "    # Initialize gradients\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_dtheta = np.zeros(n)\n",
    "    dj_dbeta = np.zeros(n)\n",
    "    dj_db = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(x[i], w) + np.dot(x[i]**2,theta)+ + np.dot(x[i]**3,beta) + b  # Predicted value\n",
    "        error = f_wb_i - y[i]  # Error term\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += error * x[i, j]  # Gradient for w[j]\n",
    "            dj_dtheta[j] += error * x[i, j]**2\n",
    "            dj_dbeta[j] += error * x[i, j]**3\n",
    "        dj_db += error  # Gradient for b\n",
    "\n",
    "    # Average the gradients\n",
    "    dj_dw /= m\n",
    "    dj_dtheta /= m\n",
    "    dj_dbeta /= m\n",
    "    dj_db/= m\n",
    "\n",
    "\n",
    "    return dj_dw, dj_db,dj_dtheta, dj_dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9uvMegwcsWN",
    "outputId": "6cfbb7b0-477a-456e-c796-1b2169cd8a3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_dw,dj_dtheta at initial w, b,theta (zeros): [-1.92084471e+03 -1.18122074e+04 -2.83832397e+03 -9.14164787e+02\n",
      " -7.63096803e+03 -5.88547603e+03 -9.57593021e+02 -1.54255794e+02\n",
      " -2.13193441e+01 -1.81230735e+05 -3.15137241e+02]\n",
      "Gradient at initial w, b (zeros): [-1.92084471e+03 -1.18122074e+04 -2.83832397e+03 -9.14164787e+02\n",
      " -7.63096803e+03 -5.88547603e+03 -9.57593021e+02 -1.54255794e+02\n",
      " -2.13193441e+01 -1.81230735e+05 -3.15137241e+02] -155.6377700388498 [-4.38318436e+04 -9.02534072e+05 -5.57854156e+04 -1.25276880e+04\n",
      " -3.75821691e+05 -2.29657750e+05 -6.02782003e+03 -4.10024277e+02\n",
      " -9.28203371e+00 -2.25608162e+08 -6.44716915e+02] [-1.61983864e+06 -6.93734729e+07 -1.16852226e+06 -2.93593305e+05\n",
      " -1.85884700e+07 -9.09740831e+06 -3.90343273e+04 -2.44664994e+03\n",
      " -8.03873283e+00 -2.94846889e+11 -1.33266037e+03]\n"
     ]
    }
   ],
   "source": [
    "m,n=x_train.shape\n",
    "initial_w = np.zeros(n)\n",
    "initial_theta=np.zeros(n)\n",
    "initial_beta=np.zeros(n)\n",
    "initial_b = 0\n",
    "\n",
    "tmp_dj_dw, tmp_dj_db,tmp_dj_dtheta,tmp_dj_dbeta = compute_gradient(x_train, y_train, initial_w, initial_b,initial_theta,initial_beta)\n",
    "print(f'dj_dw,dj_dtheta at initial w, b,theta (zeros): {tmp_dj_dw}')\n",
    "print('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db,tmp_dj_dtheta,tmp_dj_dbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mLUdICAjA2to"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in,theta_in, beta_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
    "    num_iters gradient steps with learning rate alpha.\n",
    "\n",
    "    Args:\n",
    "        x : (ndarray): Shape (m, n) Input data (features of the training examples).\n",
    "        y : (ndarray): Shape (m,) Output data (actual output values for the training examples).\n",
    "        w_in : (ndarray): Shape (n,) Initial values of parameters (weights) of the linear term.\n",
    "        theta_in : ndarray): Shape (n,) Initial values of parameters (weights) of the quadratic term.\n",
    "         \n",
    "        b_in : (scalar) Initial value of parameter (bias) of the model.\n",
    "        cost_function : function to compute cost.\n",
    "        gradient_function : function to compute the gradient.\n",
    "        alpha : (float) Learning rate.\n",
    "        num_iters : (int) Number of iterations to run gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        w : (ndarray): Shape (n,) Updated values of parameters (weights) of the model after running gradient descent.\n",
    "        b : (scalar) Updated value of parameter (bias) of the model after running gradient descent.\n",
    "        J_history : List of costs at each iteration.\n",
    "        w_history : List of weights at each iteration.\n",
    "    \"\"\"\n",
    "    m, n = x.shape\n",
    "    # Initialize parameters\n",
    "    w = copy.deepcopy(w_in)  # Avoid modifying global w within function\n",
    "    theta=copy.deepcopy(theta_in)\n",
    "    beta=copy.deepcopy(beta_in)\n",
    "    b = b_in\n",
    "\n",
    "    # History of cost and weights\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_dw, dj_db,dj_dtheta,dj_dbeta = gradient_function(x, y, w, b,theta,beta)\n",
    "\n",
    "        # Update Parameters using w, theta, b, alpha, and gradient\n",
    "        for j in range(n):\n",
    "            w[j] = w[j] - alpha * dj_dw[j]\n",
    "            theta[j] = theta[j] - alpha * dj_dtheta[j]\n",
    "            beta[j] = beta[j] - alpha * dj_dbeta[j]\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 10000000:  # Prevent resource exhaustion\n",
    "            cost = cost_function(x, y, w, b,theta,beta)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 100) == 0:\n",
    "            w_history.append(w.copy())\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):15.2f}\")\n",
    "\n",
    "    return w, b,theta,beta, J_history, w_history  # Return w and J, w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ns8IIXupA8XK",
    "outputId": "c9c74352-2bc4-48d5-f601-1b4188d2c411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 23413705339368.43\n",
      "Iteration 1500: Cost 13655732691011.09\n",
      "Iteration 3000: Cost 7964524737696.93\n",
      "Iteration 4500: Cost 4645203282812.74\n",
      "Iteration 6000: Cost 2709253127549.37\n",
      "Iteration 7500: Cost 1580135910490.01\n",
      "Iteration 9000: Cost 921593289238.27\n",
      "Iteration 10500: Cost 537507051376.65\n",
      "Iteration 12000: Cost 313493854122.55\n",
      "Iteration 13500: Cost 182841131902.18\n",
      "Iteration 15000: Cost 106639666841.92\n",
      "Iteration 16500: Cost  62196174363.17\n",
      "Iteration 18000: Cost  36275097620.79\n",
      "Iteration 19500: Cost  21156973863.86\n",
      "Iteration 21000: Cost  12339528963.64\n",
      "Iteration 22500: Cost   7196871328.46\n",
      "Iteration 24000: Cost   4197484561.43\n",
      "Iteration 25500: Cost   2448132063.86\n",
      "Iteration 27000: Cost   1427845452.48\n",
      "Iteration 28500: Cost    832776836.63\n",
      "Iteration 30000: Cost    485710969.44\n",
      "Iteration 31500: Cost    283289411.71\n",
      "Iteration 33000: Cost    165229726.49\n",
      "Iteration 34500: Cost     96372983.02\n",
      "Iteration 36000: Cost     56213201.86\n",
      "Iteration 37500: Cost     32790542.86\n",
      "Iteration 39000: Cost     19129588.06\n",
      "Iteration 40500: Cost     11162017.61\n",
      "Iteration 42000: Cost      6515037.96\n",
      "Iteration 43500: Cost      3804748.79\n",
      "Iteration 45000: Cost      2224008.63\n",
      "Iteration 46500: Cost      1302062.92\n",
      "Iteration 48000: Cost       764350.33\n",
      "Iteration 49500: Cost       450736.61\n",
      "Iteration 51000: Cost       267825.57\n",
      "Iteration 52500: Cost       161145.13\n",
      "Iteration 54000: Cost        98925.18\n",
      "Iteration 55500: Cost        62636.23\n",
      "Iteration 57000: Cost        41471.17\n",
      "Iteration 58500: Cost        29126.94\n",
      "Iteration 60000: Cost        21927.33\n",
      "Iteration 61500: Cost        17728.25\n",
      "Iteration 63000: Cost        15279.19\n",
      "Iteration 64500: Cost        13850.81\n",
      "Iteration 66000: Cost        13017.73\n",
      "Iteration 67500: Cost        12531.84\n",
      "Iteration 69000: Cost        12248.46\n",
      "Iteration 70500: Cost        12083.18\n",
      "Iteration 72000: Cost        11986.78\n",
      "Iteration 73500: Cost        11930.55\n",
      "Iteration 75000: Cost        11897.76\n",
      "Iteration 76500: Cost        11878.64\n",
      "Iteration 78000: Cost        11867.48\n",
      "Iteration 79500: Cost        11860.98\n",
      "Iteration 81000: Cost        11857.18\n",
      "Iteration 82500: Cost        11854.97\n",
      "Iteration 84000: Cost        11853.68\n",
      "Iteration 85500: Cost        11852.93\n",
      "Iteration 87000: Cost        11852.49\n",
      "Iteration 88500: Cost        11852.23\n",
      "Iteration 90000: Cost        11852.08\n",
      "Iteration 91500: Cost        11851.99\n",
      "Iteration 93000: Cost        11851.94\n",
      "Iteration 94500: Cost        11851.91\n",
      "Iteration 96000: Cost        11851.90\n",
      "Iteration 97500: Cost        11851.89\n",
      "Iteration 99000: Cost        11851.88\n",
      "Iteration 100500: Cost        11851.88\n",
      "Iteration 102000: Cost        11851.87\n",
      "Iteration 103500: Cost        11851.87\n",
      "Iteration 105000: Cost        11851.87\n",
      "Iteration 106500: Cost        11851.87\n",
      "Iteration 108000: Cost        11851.87\n",
      "Iteration 109500: Cost        11851.87\n",
      "Iteration 111000: Cost        11851.87\n",
      "Iteration 112500: Cost        11851.87\n",
      "Iteration 114000: Cost        11851.87\n",
      "Iteration 115500: Cost        11851.87\n",
      "Iteration 117000: Cost        11851.87\n",
      "Iteration 118500: Cost        11851.87\n",
      "Iteration 120000: Cost        11851.87\n",
      "Iteration 121500: Cost        11851.87\n",
      "Iteration 123000: Cost        11851.87\n",
      "Iteration 124500: Cost        11851.87\n",
      "Iteration 126000: Cost        11851.87\n",
      "Iteration 127500: Cost        11851.87\n",
      "Iteration 129000: Cost        11851.87\n",
      "Iteration 130500: Cost        11851.87\n",
      "Iteration 132000: Cost        11851.87\n",
      "Iteration 133500: Cost        11851.87\n",
      "Iteration 135000: Cost        11851.87\n",
      "Iteration 136500: Cost        11851.87\n",
      "Iteration 138000: Cost        11851.87\n",
      "Iteration 139500: Cost        11851.87\n",
      "Iteration 141000: Cost        11851.87\n",
      "Iteration 142500: Cost        11851.87\n",
      "Iteration 144000: Cost        11851.87\n",
      "Iteration 145500: Cost        11851.87\n",
      "Iteration 147000: Cost        11851.87\n",
      "Iteration 148500: Cost        11851.87\n",
      "w, b found by gradient descent: [-2.07119560e-11 -1.06305449e-10 -2.51019881e-11 -8.95663350e-12\n",
      " -6.74602387e-11 -5.41060801e-11 -8.27032010e-12 -1.35851814e-12\n",
      " -2.04066154e-13 -1.84405721e-09 -2.84725353e-12] -40.0 [5.19994233e-05 1.99188733e-06 3.95201316e-08 2.49998750e-05\n",
      " 7.99967385e-05 8.99978623e-05 4.09999503e-05 1.99999997e-04\n",
      " 1.20000000e-04 3.37490393e-04 4.49999416e-06] [ 9.99761833e-05 -4.23188386e-07  9.02653527e-08  4.71720845e-08\n",
      "  2.99841454e-04  2.99991445e-03  1.99695058e-07  9.99812078e-08\n",
      "  3.99999933e-07 -2.36424190e-07  1.49999999e-03]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "m,n=x_train.shape\n",
    "iinitial_w = np.array([-5.8, 2.7, 0.741, -10.8, 10.5, -12.54,8.98, -1.04, -0.75, 0.41, -0.7])\n",
    "initial_theta=np.array([0.000052, 0.000002, 0.00000004, 0.000025, 0.00008, 0.00009, 0.000041, 0.0002, 0.00012, 0.00034, 0.0000045])\n",
    "initial_beta=np.array([0.0001, 0.0000002, 0.0000001, 0.00000005, 0.0003, 0.003, 0.0000002, 0.0000001, 0.0000004, 0.0035, 0.0015])\n",
    "initial_b=-40\n",
    "\n",
    "# Define the learning rate and number of iterations\n",
    "alpha = 0.000000000000000000000047\n",
    "iterations = 150000\n",
    "# Perform gradient descent\n",
    "w, b, theta,beta,_,_ = gradient_descent(x_train, y_train, initial_w, initial_b,initial_theta, initial_beta,compute_cost, compute_gradient, alpha, iterations)\n",
    "\n",
    "print(\"w, b found by gradient descent:\", w, b,theta,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "wt_tfNUsWfvA",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x_test is: (197, 11)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of x_test is:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "9MSyCWFmVCil"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280.36007076140356 223.154376\n",
      "180.06788375874874 207.188186\n",
      "264.88442324363547 133.971542\n",
      "275.2096884740223 219.292928\n",
      "294.9382286769084 174.687108\n",
      "253.74936193210314 57.1057396\n",
      "286.195354637433 175.320837\n",
      "147.17072017078868 249.387055\n",
      "289.075058981461 44.9859809\n",
      "281.41257844505486 34.5454599\n",
      "297.9015534567165 44.2105264\n",
      "107.2159049885533 166.013301\n",
      "56.949775090959896 436.789023\n",
      "253.7405663441075 247.122796\n",
      "310.5812556554082 182.545969\n",
      "275.7198572281167 223.451568\n",
      "302.2530657168075 17.0370557\n",
      "280.41484304097725 200.170248\n",
      "213.7348630241186 244.041376\n",
      "56.943518786018615 428.298665\n",
      "284.81978605451246 90.9434057\n",
      "135.61419100882688 295.946534\n",
      "101.84331613893126 342.998919\n",
      "296.36895267828606 157.246183\n",
      "241.32186073999165 164.795386\n",
      "222.11674135052758 81.6187988\n",
      "239.1029952004382 137.313404\n",
      "246.7338300550883 98.6450947\n",
      "194.55783241876668 221.545913\n",
      "113.10956612781531 247.130103\n",
      "286.1920992174642 183.966135\n",
      "296.08805157542145 175.207607\n",
      "79.57071529240554 104.485971\n",
      "225.18872367136493 237.963632\n",
      "280.35681534143475 229.259754\n",
      "308.2780699456152 142.324015\n",
      "233.2051032418439 154.779523\n",
      "303.00916403138564 173.366842\n",
      "239.10925150537946 125.942326\n",
      "213.74437474902868 236.362441\n",
      "330.19689686215 163.485678\n",
      "294.94774040181846 152.354279\n",
      "165.32307733282184 249.310148\n",
      "239.36748937691323 114.750256\n",
      "248.75128617500874 211.02923\n",
      "81.74651533860545 254.794437\n",
      "123.07957196379665 179.411665\n",
      "305.2020421977381 107.907488\n",
      "135.17850760539727 303.780146\n",
      "269.15867930807497 149.05531\n",
      "280.4210993459185 191.17593\n",
      "126.31781752516669 408.736237\n",
      "228.56217709978048 62.9240493\n",
      "273.4048513319892 61.8265134\n",
      "338.18739891613393 70.2102987\n",
      "241.56976888958374 142.88996\n",
      "242.71685070472788 288.929171\n",
      "112.16175697035582 183.692632\n",
      "343.47310613229683 99.3641941\n",
      "277.8035380405832 28.2270584\n",
      "307.5928203687095 32.1597258\n",
      "164.47689739646998 57.3485701\n",
      "274.76123792571383 238.651989\n",
      "291.80652017693984 178.720905\n",
      "126.69342746808634 326.379167\n",
      "216.21982479129676 53.9046554\n",
      "162.79487508900723 197.806561\n",
      "289.6188198806573 109.940767\n",
      "269.1649356130162 138.150498\n",
      "256.53541452771304 142.398004\n",
      "332.8476442642319 77.3269801\n",
      "306.77452376221436 174.744232\n",
      "202.95593025200083 144.793288\n",
      "74.17812061554966 240.321946\n",
      "140.50873587110274 179.573213\n",
      "302.37760540932373 137.164486\n",
      "193.2554022190053 160.99919\n",
      "198.58890056596607 131.344405\n",
      "286.5630799688417 248.44263\n",
      "140.51824759601283 170.874127\n",
      "260.19837997222226 23.9673878\n",
      "268.313327208679 219.004416\n",
      "294.19008637360895 181.066859\n",
      "286.5663353888105 243.714498\n",
      "128.7273414683937 293.718174\n",
      "296.359440953376 167.210396\n",
      "247.107884213629 165.852115\n",
      "188.0394599001138 38.0714492\n",
      "279.5468319549124 110.185037\n",
      "318.20607972807335 217.678058\n",
      "252.69855275307313 181.654324\n",
      "181.42299418351416 262.94273\n",
      "318.2155914529834 195.174678\n",
      "216.22933651620684 44.3779809\n",
      "319.6187263948817 58.6333301\n",
      "81.75277164354672 251.785857\n",
      "294.199598098519 163.701809\n",
      "137.72039852743478 279.166532\n",
      "234.55352087565447 24.2898121\n",
      "346.97828732885205 70.7950056\n",
      "290.77393506982656 123.052656\n",
      "240.09489161966854 32.8929284\n",
      "165.93750438687204 244.002816\n",
      "373.95393241695865 64.6711163\n",
      "137.72665483237606 277.285736\n",
      "302.26257744171755 4.65812162\n",
      "315.37533898552493 136.342397\n",
      "209.2995260025639 61.7411681\n",
      "197.76668315527817 224.24754\n",
      "239.87230396287202 204.219693\n",
      "169.5458109747682 267.950784\n",
      "144.7454421293597 187.29848\n",
      "319.6124700899405 65.48975\n",
      "199.92433730046056 180.072041\n",
      "373.95067699698984 76.0949717\n",
      "274.76449334568264 231.668368\n",
      "274.62323168016496 49.343001\n",
      "244.74125298012513 261.880061\n",
      "264.3626735793041 68.3101602\n",
      "163.61044774409288 202.726578\n",
      "147.3330228014763 137.594684\n",
      "319.66012580397535 124.455059\n",
      "234.68952992658637 191.527882\n",
      "284.8102743296024 98.1588502\n",
      "239.09973978046938 147.674115\n",
      "257.9367654852584 97.4667352\n",
      "251.64712597037848 252.696691\n",
      "273.39533960707917 75.8015429\n",
      "104.92734532138785 265.7585\n",
      "0.6791083248420703 169.850334\n",
      "343.4668498273556 108.638951\n",
      "167.18553713695655 242.487828\n",
      "260.192123667281 31.7477501\n",
      "233.1988469369026 159.285135\n",
      "125.29843750335012 208.129133\n",
      "175.87400054495131 182.781336\n",
      "241.31560443505037 174.44631\n",
      "172.4149149472527 181.377187\n",
      "319.9282597619611 141.045447\n",
      "123.08582826873793 171.957654\n",
      "289.34644835941555 67.5384145\n",
      "220.96028028293946 219.518801\n",
      "280.1009779163293 118.693259\n",
      "313.22656481390186 116.449583\n",
      "133.8880257483398 236.614484\n",
      "329.7677979856702 192.176408\n",
      "292.72424269669125 124.887028\n",
      "220.95076855802938 232.423647\n",
      "-50.66985788895543 123.484764\n",
      "232.0834917665401 22.1861716\n",
      "353.33036048931405 159.608876\n",
      "67.17138432653047 84.8935651\n",
      "337.3380367409376 173.676262\n",
      "296.09130699539025 166.408317\n",
      "310.58751196034945 173.421513\n",
      "350.2037932173961 139.188629\n",
      "178.5376169650854 69.6214875\n",
      "259.1409876173536 164.211847\n",
      "284.79027216301057 45.7624249\n",
      "246.72757375014703 105.088383\n",
      "157.1284001825438 12.4643444\n",
      "265.13233139322756 111.514921\n",
      "350.19753691245484 151.668\n",
      "219.15670103269088 32.7054396\n",
      "235.32499274787895 240.292179\n",
      "322.7515499717409 63.807184\n",
      "299.6213417645457 58.7446198\n",
      "277.3126405726614 205.014769\n",
      "234.69904165149646 177.314467\n",
      "223.23607280522603 144.088565\n",
      "144.75169843430098 183.487636\n",
      "207.38604528362544 166.541559\n",
      "193.24589049409522 170.93279\n",
      "328.4898756991997 117.235871\n",
      "140.27760667500996 71.8010685\n",
      "263.3564460381178 120.695862\n",
      "225.17921194645484 241.093112\n",
      "304.91857479801723 165.062306\n",
      "296.3626963733448 162.872211\n",
      "237.72500496619585 123.052314\n",
      "276.19896866461005 61.2762741\n",
      "210.60529486722825 229.330898\n",
      "251.74448729067237 146.65583\n",
      "151.19120474951006 186.754611\n",
      "226.5310053655272 95.1329816\n",
      "287.18335008458973 44.8597129\n",
      "315.46624259017796 99.1274495\n",
      "353.32710506934524 171.265195\n",
      "306.7624816433333 71.1217494\n",
      "249.5191665137838 36.2783329\n",
      "133.89428205328107 231.60607\n",
      "262.1424312941457 49.783011\n",
      "246.3804819708737 258.71213\n",
      "116.70253156148358 128.826424\n",
      "210.8567476665848 164.470379\n",
      "167.18228171698775 245.198155\n",
      "249.52542281872508 29.9618589\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='Result_Cubic_Regression2.xlsx' target='_blank'>Result_Cubic_Regression2.xlsx</a><br>"
      ],
      "text/plain": [
       "C:\\Users\\Mafi\\Data file\\Result_Cubic_Regression2.xlsx"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "k=x_test.shape[0]\n",
    "data=[]\n",
    "for i in range(k):\n",
    "        f_wb_i = b  # Start with the bias term\n",
    "        for j in range(11):  # Sum over all features\n",
    "            f_wb_i += w[j] * x_test[i, j]+theta[j] * x_test[i, j]**2 + beta[j] * x_test[i, j]**3\n",
    "        print(f_wb_i, y_test[i])\n",
    "        row= [f_wb_i,y_test[i]]\n",
    "        data.append(row)\n",
    "df=pd.DataFrame(data,columns=['Predicted','Actual'])\n",
    "file_name='Result_Cubic_Regression2.xlsx'\n",
    "df.to_excel(file_name, index=False)\n",
    "FileLink(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
